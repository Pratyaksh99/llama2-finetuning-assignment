# Core dependencies for LLaMA-2 fine-tuning with LoRA

# Transformers and related
transformers>=4.35.0
accelerate>=0.24.0
peft>=0.6.0
bitsandbytes>=0.41.0
scipy>=1.11.0

# Dataset and tokenization
datasets>=2.14.0
sentencepiece>=0.1.99
protobuf>=3.20.0

# Training and optimization
torch>=2.0.0
tqdm>=4.65.0

# Evaluation
pandas>=2.0.0
numpy>=1.24.0

# Visualization
matplotlib>=3.7.0
seaborn>=0.12.0

# Utilities
huggingface-hub>=0.17.0
einops>=0.7.0

# Optional: For extended evaluation
# alpaca-eval>=0.2.0
# fastchat>=0.2.0
